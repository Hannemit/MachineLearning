\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear Regression}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Gradient descent}{1}{subsection.1.1}}
\newlabel{Eq: Definition X matrix}{{3}{1}{Gradient descent}{equation.1.3}{}}
\newlabel{Eq: Cost function linear regression}{{5}{1}{Gradient descent}{equation.1.5}{}}
\newlabel{Eq: Theta update rule}{{6}{1}{Gradient descent}{equation.1.6}{}}
\newlabel{Eq: Unregularized theta update rule}{{8}{1}{Gradient descent}{equation.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Feature Scaling}{2}{subsection.1.2}}
\newlabel{Eq: Mean norm}{{9}{2}{Feature Scaling}{equation.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Choosing a learning rate}{2}{subsection.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Convergence} {This is determinsitic gradient descent so if done properly the cost should always go down. \textbf  {A)} The cost decreases at every iteration and the algorithm is sucessfully approaching the optimum. \textbf  {B)} The cost is diverging and it is likely that this is caused by a learning rate that is too large. \textbf  {C)} This oscillating behaviour of the cost as function of the number of iterations is also often an indicating of a too large learning rate. }}}}{2}{figure.1}}
\newlabel{Fig: Learning rate figure}{{1}{2}{\footnotesize {\textbf {Convergence} {This is determinsitic gradient descent so if done properly the cost should always go down. \textbf {A)} The cost decreases at every iteration and the algorithm is sucessfully approaching the optimum. \textbf {B)} The cost is diverging and it is likely that this is caused by a learning rate that is too large. \textbf {C)} This oscillating behaviour of the cost as function of the number of iterations is also often an indicating of a too large learning rate. }}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Choosing features}{2}{subsection.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Polynomial regression}{2}{subsubsection.1.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Normal equation}{3}{subsection.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}When use the normal equation}{3}{subsubsection.1.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}What if cannot invert?}{3}{subsubsection.1.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Classification: Logistic Regression}{4}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Two classes}{4}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Logistic Regression A)} Given some feature input x, we get the logistic thingie as an output and this gives us the probability that the output is 1. \textbf  {B)} Decision boundary. }}}{4}{figure.2}}
\newlabel{Fig: Logistic regression}{{2}{4}{\footnotesize {\textbf {Logistic Regression A)} Given some feature input x, we get the logistic thingie as an output and this gives us the probability that the output is 1. \textbf {B)} Decision boundary. }}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Examples}{4}{subsubsection.2.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Fitting parameters}{5}{subsubsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Pick nice cost function A, B)} If we pick the same cost function as for linear regression, then our overall $J(\theta )$ as a function of $\theta $ will be quite complex and non-convex. This is not what we want, we want a nice looking $J(\theta )$. \textbf  {C, D)} When picking the cost function as specified in Eq. \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {Eq: Logistic cost function}\unskip \@@italiccorr )}}, we get some nice properties. }}}{5}{figure.3}}
\newlabel{Fig: Convex}{{3}{5}{\footnotesize {\textbf {Pick nice cost function A, B)} If we pick the same cost function as for linear regression, then our overall $J(\theta )$ as a function of $\theta $ will be quite complex and non-convex. This is not what we want, we want a nice looking $J(\theta )$. \textbf {C, D)} When picking the cost function as specified in Eq. \eqref {Eq: Logistic cost function}, we get some nice properties. }}{figure.3}{}}
\newlabel{Eq: Logistic cost function}{{14}{5}{Fitting parameters}{equation.2.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Simpler cost function}{5}{subsubsection.2.1.3}}
\newlabel{Eq: Logistic final cost function 2 classes}{{15}{5}{Simpler cost function}{equation.2.15}{}}
\newlabel{Eq: Final Final logistic cost function}{{16}{6}{Simpler cost function}{equation.2.16}{}}
\newlabel{Eq: Theta update logistic unregularized}{{17}{6}{Simpler cost function}{equation.2.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Beyond gradient descent}{6}{subsubsection.2.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Multiple Classes}{6}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Logistic Regression multiple classes} Take all the training examples belonging to a specific class and now treat all of the other data as belonging to a single different class. So here our original classes are red, blue and green, but we treat it as three classification problems: red vs the rest, blue vs the rest, and green vs the rest. Each of the individual classifications allows us to calculate the probability of belonging to that specific class when a new data point is presented. }}}{6}{figure.4}}
\newlabel{Fig: Multiple Classes}{{4}{6}{\footnotesize {\textbf {Logistic Regression multiple classes} Take all the training examples belonging to a specific class and now treat all of the other data as belonging to a single different class. So here our original classes are red, blue and green, but we treat it as three classification problems: red vs the rest, blue vs the rest, and green vs the rest. Each of the individual classifications allows us to calculate the probability of belonging to that specific class when a new data point is presented. }}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Overfitting and underfitting}{7}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Under- and overfitting} The plots speak for themselves.}}}{7}{figure.5}}
\newlabel{Fig: UnderOver}{{5}{7}{\footnotesize {\textbf {Under- and overfitting} The plots speak for themselves.}}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}How do we know we are overfitting?}{7}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Regularization}{7}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Examples}{7}{subsubsection.3.2.1}}
\newlabel{Eq: Regularized linear regression}{{19}{7}{Examples}{equation.3.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Regularized linear regression}{8}{subsubsection.3.2.2}}
\newlabel{Eq: Regularized theta update}{{20}{8}{Regularized linear regression}{equation.3.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Non-invertibility}{8}{subsubsection.3.2.3}}
\newlabel{Sec: Regularization and invertibility}{{3.2.3}{8}{Non-invertibility}{subsubsection.3.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Regularized Logistic Regression}{8}{subsubsection.3.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Networks}{8}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Neural network representation A)} A cartoon of actual neurons. \textbf  {B)} The basic units of a neural network and their relation to actual neurons. \textbf  {C)} An example of a neural network with a single hidden layer and a single output neuron. }}}{9}{figure.6}}
\newlabel{Fig: Neuron}{{6}{9}{\footnotesize {\textbf {Neural network representation A)} A cartoon of actual neurons. \textbf {B)} The basic units of a neural network and their relation to actual neurons. \textbf {C)} An example of a neural network with a single hidden layer and a single output neuron. }}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Some intuition}{10}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Forward propagation}{10}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Back propagation}{10}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}How to improve}{10}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Training, (cross) validation and test sets}{10}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Over- and underfitting}{10}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Choose the regularization parameter $\lambda $}{10}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Diagnostic for over- and underfitting} By comparing the errors from the training and validation sets, one can determine whether it is likely that we're over- or underfitting.}}}{11}{figure.7}}
\newlabel{Fig: OverUnder Diagnostic}{{7}{11}{\footnotesize {\textbf {Diagnostic for over- and underfitting} By comparing the errors from the training and validation sets, one can determine whether it is likely that we're over- or underfitting.}}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Learning curves}{11}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}So, what do we do?}{11}{subsection.5.5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Overview of when to use certain methods to improve performance} There are several ways in which the performance of our model can be improved, some are listed here. We also describe the situations in which these `improvements' would or wouldn't work. }}}{11}{table.1}}
\newlabel{Tab: Improving performance}{{1}{11}{\footnotesize {\textbf {Overview of when to use certain methods to improve performance} There are several ways in which the performance of our model can be improved, some are listed here. We also describe the situations in which these `improvements' would or wouldn't work. }}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Good way to start}{11}{subsection.5.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Learning curve} Here we compare learning curves in `good' (A,B), underfitting (C,D) and overfitting (E, F) scenarios. The figures on the right are meant to motivate some of the shapes of the learning curves on the left. }}}{12}{figure.8}}
\newlabel{Fig: Learning curve}{{8}{12}{\footnotesize {\textbf {Learning curve} Here we compare learning curves in `good' (A,B), underfitting (C,D) and overfitting (E, F) scenarios. The figures on the right are meant to motivate some of the shapes of the learning curves on the left. }}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Skewed classes}{12}{subsection.5.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Does more data help?}{13}{subsection.5.8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Support Vector Machines}{13}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Cost function for SVM}{14}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Kernels}{14}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Gaussian kernel features} This cartoon illustrates how the features of a SVM with gaussian kernels are constructed. Consider the red training example. The features corresponding to this red example are the values of the gaussians centered at each of the training examples (including itself) at the position of the red example itself. These are indicated by the red circles (and the gaussian centered at the red example itself represents a feature value of $1$). The same is true for the other training examples. For example, if we denote the red, green and blue training examples by $x^{(1)}, x^{(2)}$ and $x^{(3)}$, then $f_3^{(1)}$ has a small value whereas $f_2^{(1)}$ is larger. Each training example here is represented by a $4$-dimensional feature vector. }}}{15}{figure.9}}
\newlabel{Fig: Gaussian Kernel}{{9}{15}{\footnotesize {\textbf {Gaussian kernel features} This cartoon illustrates how the features of a SVM with gaussian kernels are constructed. Consider the red training example. The features corresponding to this red example are the values of the gaussians centered at each of the training examples (including itself) at the position of the red example itself. These are indicated by the red circles (and the gaussian centered at the red example itself represents a feature value of $1$). The same is true for the other training examples. For example, if we denote the red, green and blue training examples by $x\ind {1}, x\ind {2}$ and $x\ind {3}$, then $f_3\ind {1}$ has a small value whereas $f_2\ind {1}$ is larger. Each training example here is represented by a $4$-dimensional feature vector. }}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Note on feature confusion}{15}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Model selection tips}{15}{subsection.6.4}}
\@writefile{toc}{\contentsline {section}{\numberline {7}K-means clustering}{16}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}The algorithm}{16}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Cost function}{16}{subsubsection.7.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {K-means clustering cost function} Here we illustrate the terms in the cost function of the K-means clustering algorithm. For each data point, the squared distance between this point and its cluster centroid (i.e. the centroid of the cluster to which it is currently assigned) is calculated. All these distances are added to give the cost of the current configuration. }}}{16}{figure.10}}
\newlabel{Fig: K clustering cost}{{10}{16}{\footnotesize {\textbf {K-means clustering cost function} Here we illustrate the terms in the cost function of the K-means clustering algorithm. For each data point, the squared distance between this point and its cluster centroid (i.e. the centroid of the cluster to which it is currently assigned) is calculated. All these distances are added to give the cost of the current configuration. }}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Randomly initialising cluster centroids}{17}{subsubsection.7.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}How do we choose K?}{17}{subsection.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Large data sets}{17}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Stochastic gradient descent}{17}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Mini-batch gradient descent}{18}{subsection.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Principal Component Analysis}{18}{section.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}The algorithm}{18}{subsection.9.1}}
\newlabel{Eq: Cov matrix}{{31}{18}{The algorithm}{equation.9.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}How to go back to our original data?}{19}{subsection.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}How do we choose the number of principal components?}{19}{subsection.9.3}}
\newlabel{Eq: Variance retained PCA}{{34}{19}{How do we choose the number of principal components?}{equation.9.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Example of when we could use PCA}{19}{subsection.9.4}}
